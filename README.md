# ğŸš€ Pipeline ETL com Arquitetura MedalhÃ£o

Este projeto foi desenvolvido como um desafio prÃ¡tico de Engenharia de Dados, focado na implementaÃ§Ã£o da **Arquitetura MedalhÃ£o** para transformar dados crus em uma visÃ£o analÃ­tica consolidada. Indo alÃ©m do bÃ¡sico, implementei rotinas de **Data Quality**, normalizaÃ§Ã£o e anÃ¡lise exploratÃ³ria.

---

## ğŸ—ï¸ A Arquitetura e o Fluxo de Dados

O pipeline segue a separaÃ§Ã£o lÃ³gica de responsabilidades:

* **Camada Bronze (Raw):** Armazenamento fiel dos dados brutos recebidos (`CSV`, `JSON`).
* **Camada Silver (Validated):** Processo de limpeza e conversÃ£o para formato **Parquet**, otimizando performance.
* **Camada Gold (Enriched):** Camada de consumo com **Inner Join** entre UsuÃ¡rios e LocalizaÃ§Ã£o (CEP), gerando uma visÃ£o pronta para BI.

---

## ğŸ› ï¸ Tecnologias e Ferramentas

* **Linguagem:** Python 3.x
* **Processamento:** Pandas
* **Armazenamento:** Parquet (Formatos colunares)
* **Banco de Dados:** PostgreSQL (Docker)
* **VisualizaÃ§Ã£o:** Matplotlib & Seaborn
* **Ambiente:** Jupyter Notebooks & VS Code

---

## ğŸ’¡ Meus Diferenciais (Data Quality)

Para elevar o nÃ­vel tÃ©cnico, implementei:

* **NormalizaÃ§Ã£o de GÃªnero:** CorreÃ§Ã£o de strings (ex: "F " vs "F") para evitar duplicidade.
* **OrdenaÃ§Ã£o NumÃ©rica:** Uso de `CAST` de IDs para ordenaÃ§Ã£o correta na Gold.
* **Data Visualization:** Dashboard completo no `data-view.ipynb`.
* **Boas PrÃ¡ticas:** `.gitignore` profissional e `requirements.txt`.

---

## ğŸš€ Como Rodar o Projeto

1. **Clone o repositÃ³rio:**
   ```bash
   git clone [https://github.com/seu-usuario/seu-repositorio.git](https://github.com/seu-usuario/seu-repositorio.git)
   cd seu-repositorio

2. **Crie e ative seu ambiente virtual:**
   ```bash
   python -m venv .venv
    # Windows:
    .venv\Scripts\activate
    # Linux/Mac:
    source .venv/bin/activate

3. **Instale as dependÃªncias:**
   ```bash
   pip install -r requirements.txt

4. **Suba o banco e rode o pipeline:**
    ```bash
    docker-compose up -d
    python populate_db.py
    python normalize_data.py

---

## ğŸ“ˆ PrÃ³ximos Passos

Pretendo integrar o arquivo products.json para criar uma Tabela Fato de Vendas, permitindo analisar o ticket mÃ©dio por regiÃ£o.